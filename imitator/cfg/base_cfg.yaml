# task_name: ${task.name}

project:
  task: "can"
  algo : "bc"
  model: "transformer"

  exp_name: "${project.algo}_${project.model}_${project.task}"

  train:
    batch_size: 16
    num_epochs: 1000
    seed: 42

hydra:
  verbose: False
  run:
    dir: runs/${project.task}/${project.algo}_${project.model}/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: runs
    subdir: ${project.exp_name}/${hydra.job.num}

# network:
#   policy:
#     model: "TransformerActor"
#     model_path: null
#     transformer:
#       type: "GPT"
#       transformer_num_layers: 6
#       transformer_num_heads: 8
#       transformer_embed_dim: 512
#       context_length: 20
#       attn_dropout: 0.1
#       block_dropout: 0.1
#       activation: "gelu"
#     mlp_decoder:
#       layer_dims: [128, 128, 128]
#       activation: "ReLU"
#       squash_output: True
#     train:
#       lr: 1e-4
#       lr_scheduler: True
#       optimizer: "Adam"
#       batch_size: 128
#       seq_length: 10 # equal to rnn seq_length if RNNActor
#       weight:
#         l1: 0.0
#         l2: 1.0


device: "cuda:0"

ros:
  rate: 10
  action_type: "end_effector" # end_effector (x,y,z,r,p,y), angle_vector (seq of joint angle)
  joints: ["test"]
  message_filters:
    slop: 0.1
    queue_size: 1000
