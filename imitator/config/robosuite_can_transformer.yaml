obs: # input into policy
  robot0_eef_pos:
    modality: "FloatVectorModality"
    normalize: True
    obs_encoder:
      input_dim: 3
      output_dim: 3
      layer_dims: null
      activation: "ReLU" # [ReLU, Tanh...]
  robot0_eef_quat:
    modality: "FloatVectorModality"
    normalize: True
    obs_encoder:
      input_dim: 4
      output_dim: 4
      layer_dims: null
      activation: "ReLU" # [ReLU, Tanh...]
  robot0_gripper_qpos:
    modality: "FloatVectorModality"
    normalize: True
    obs_encoder:
      input_dim: 2
      output_dim: 2
      layer_dims: null
      activation: "ReLU" # [ReLU, Tanh...]
  object:
    modality: "FloatVectorModality"
    normalize: True
    obs_encoder:
      input_dim: 14
      output_dim: 14
      layer_dims: null
      activation: "ReLU" # [ReLU, Tanh...]
actions:
  modality: "FloatVectorModality"
  dim: 7
  normalize: True

network:
  policy:
    model: "TransformerActor"
    model_path: null
    transformer:
      type: "GPT"
      transformer_num_layers: 6
      transformer_num_heads: 8
      transformer_embed_dim: 512
      context_length: 20
      embed_dropout: 0.1
      attn_dropout: 0.1
      block_dropout: 0.1
      activation: "gelu"
    mlp_decoder:
      layer_dims: []
      activation: "ReLU"
      squash_output: True
    gmm:
      enabled: True
      modes: 5
      min_std: 0.0001
      low_noise_eval: True
      use_tanh: False
      std_activation: "F.softplus" # or torch.exp
    train:
      num_epochs: 1000
      batch_size: 4096
      criterion: "MSELoss"
      lr: 1e-4
      lr_scheduler: True
      supervise_all_steps: True
      optimizer: "AdamW"
      weight_decay: 0.01
      max_grad_norm: null # null or 1, 3, 5,...
      weight:
        l1: 0.0
        l2: 1.0

dataset:
  dataset_keys: ["actions"]
  load_next_obs: True
  frame_stack: 20
  pad_frame_stack: True
  pad_seq_length: True
  get_pad_mask: False
  goal_mode: null
  hdf5_cache_mode: "all"
  hdf5_use_swmr: True
